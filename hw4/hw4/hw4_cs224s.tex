\documentclass{article}
\usepackage{amsmath,amssymb,epsf}
\usepackage{url}
\usepackage{hyperref}
%\pagestyle{empty} 
\usepackage{color}

\addtolength{\textwidth}{1.0in}
\addtolength{\textheight}{0.5in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\evensidemargin}{-0.5in}
\newcommand{\ruleskip}{\bigskip\hrule\bigskip}
%\newcommand{\nodify}[1]{{\sc #1}}
%\newcommand{\points}[1]{{\textbf{[#1 points]}}}
%\newcommand{\subquestionpoints}[1]{{[#1 points]}}
%\newenvironment{answer}{{\bf Answer:} \sf }{}%

%\newcommand{\bitem}{\begin{list}{$\bullet$}%
%{\setlength{\itemsep}{0pt}\setlength{\topsep}{0pt}%
%\setlength{\rightmargin}{0pt}}}
%\newcommand{\eitem}{\end{list}}

%\setlength{\parindent}{0pt} \setlength{\parskip}{0.5ex}
%\setlength{\unitlength}{1cm}

\setlength{\parskip}{3pt}

\renewcommand{\Re}{{\mathbb R}}

\begin{document}

\pagestyle{myheadings} 

\begin{center}
\Large{{\bf Homework 4: Open Domain Dialogue Generation}}\\
\large{Due: 24th May 2017 at 11:59 PM PST} 
\end{center}
\ruleskip

\noindent {\bf {\Large Overview} }\\
The goal of this homework is to give you a basic sense of how the up-to-date open-domain dialogue generation algorithms works. 
You will have hand-on experiences on different response generation algorithms: an IR (information retrieval) based extractive generation model and an 
abstractive response generation model using neural networks. We will also briefly touch on how to design  metrics for automatic dialogue evaluation. 

Start early, and come to the TA office's hours for help if needed. Have fun! 
~~\\~~\\~~\\
\noindent {\bf {\Large Problem Sketch} }\\
The problem of open-domain dialogue generation
 can be formalized as follows: 
the input to your system is dialogue history\footnote{In this assignment, we consider a simple case where we approximate dialogue history using the latest dialogue utterance}. 
Given this input dialogue history\footnote{in this assignment, dialogue history, source, input, source input and message all mean the dialogue history inputted to your system. They thus mean the same thing. Also, output, response and target are interchaneable}, your system needs to output a sentence  that is coherent and meaningful to your input, for example, {\it ``I am 16."}
in response to {\it how old are you?}
. 

We will use BLEU[1] for evaluations. To get BLEU scores, run the following script:
\begin{center}
python bleu.py reference$\_$file $<$ candidate$\_$file 
\end{center}
where each line in candidate$\_$file consists a source input (dialogue history) and the response generated by your system, separated by a $|$,  for example:
\begin{center}
how old are you $|$ I don't know .
\end{center}
and each line in reference$\_$file consists {\bf the same} source input and the response generated by humans.
Lines in  reference$\_$file and candidate$\_$file with the same line number should have the {\bf same} input. 

Run the script using the given files:
\begin{center}
python bleu.py data/dev.txt  $<$ data/example.txt
\end{center}

After running the script, you should be able to get a BLEU score of 0.99. 
\section{the IR based model}
You are given a big pool of source-target pairs $P=\{s_i, t_i\}$ stored in the file {\it data/pool.txt}. 
Given a new input $s$, your goal is to select a source-target pair $(s',t')$ from $P$, where you will directly copy $t'$ as a response to the input $s$.
The task here is to design an algorithm on how to select  a source-target pair from the pool $P$. 

You will find two files in the folder: $dev.txt$ (development set) and $test.txt$ (test set), each line of which consists of a source (dialogue history) and a target (human-generated response)
separated by a $|$.
For each dialogue history in $test.txt$, pick a response from $pool.txt$ and store your input-output pair in $test\_decode.txt$.
Then run the evaluation code: 
\begin{center}
python bleu.py data/test.txt $<$ data/test$\_$decode.txt 
\end{center}
You can use dev.txt to tune your model. But you {\bf should not} tune your model on test.txt. 

\noindent{\bf Hint on Model Design}: You can use whichever algorithm you like. The most basic idea is that, you want to find the most related source in $P$ to your new input $s$, and then you use the response to $s'$, which is $t'$, as your response to $s$. There are many ways to compute the relatedness between two natural language utterances: the simplest one is number of overlapping words with sentence length normalization; a bit more sophisticated one is using tf-idf\footnote{\url{https://en.wikipedia.org/wiki/Tf-idf}. You can treat each sentence as a document when computing idf.}; or use the cosine similarity between two sentence vector representations.\footnote{the representation of a sentence can be obtained by averaging the representations of its constituent words. You can download Glove [2] word representations here; \url{http://cs.stanford.edu/~bdlijiwei/word_vector.tar}. }


Your starting code is in {\it q1.py}. Theoretically, you only need to change function $SelectOutput$, which takes as input a new-input, source$\_$list and target$\_$list in $pool.txt$, and returns a selected output.
Write a report on how your algorithm works, and report the BLEU scores on both the development set and the test set. 

\section{Neural Generation Models for Dialogue Generation}
Neural sequence-to-sequence models (seq2seq)  [3] have been widely used in dialogue generation. 
Please refer to
slides from CS224n 
 \url{http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture10.pdf} for more details about seq2seq models.
At training time, 
the model is trained on a big corpus of source-target pairs by sequentially predicting each token within each target. At test time, given a source message input $s$, you want to generate (decode) the most probable response $t$ using the trained model. 
Decoding 
can be considered as 
a search problem where standard search algorithms can be used, for example, the beam search model.   
Training sequence-to-sequence models is very time-intensive (it usually takes at least weeks to train). You thus will not be asked to train the models yourself. 

What you need to do is as follows:
for each input $s$, you are given a big list of response candidates generated by a pre-trained sequence-to-sequence model. 
This list is conventionally called an {\it N-best} list. Each candidate is associated with a few features, for example, candidate length, $\log$ p(t$|$s) (the log probability of a target $t$ given the source $s$), log p(s$|$t)
(the log probability of a source $s$ given the target $t$), and you are also free to design whatever features you think are useful. 
Your job is to design a score function that scores each candidate using the  pre-given features (and features that you design yourself). 
Given an input, you can 
pick an utterance from the N-best list and use it as the response using this score function.
 This process is conventionally called {\it reranking}.

The N-best list files are stored in $data/dev\_N\_best$ and $data/test\_N\_best$. 
Each line in the file is as follows:
\begin{center}
source index $\$$ candidate length $\$$ logp(t$|$s)$\$$ logp(s$|$t) $\$$ source $\$$ candidate
\end{center}

Use the code in $q2.py$. Theoretically, you only need to change the function $computeScore$. 
The simplest case is that your returned score is just $\log$ p(t$|$s). But you want to design a better score function than leads to better BLEU scores.
You should at least get a BLEU score of 1.10 on the dev set.
Again, you {\bf should not} tune your model on test.txt.

Write a short report on how you design your function,  weight values associated with different features, and explain why they lead to better BLEU scores. 

\section{Adversarial Evaluation}
{\bf background}:
So far, we have been using BLEU for evaluation. 
BLEU computes the amount
of word-overlap between the machine-generated response and
the ground-truth response. This word-overlapping can to some extent measures the quality of the proposed response, but an observant researcher can easily spot the fundamental  flaw with BLEU: there are way more than a single way to respond to an input, and computing the word-overlapping between {\bf one} proposed response and the the ground-truth response is both not enough and unreliable. 

Many alternative evaluation metrics have been proposed, one of which is {\it adversarial evaluation}. The basic idea of the {\it adversarial evaluation} draws intuitions from the  Turing Test. In the Turing test, 
a machine is said to be able pass the test if it can fool a human evaluator into believing that it (the machine) is a human, based on the conversational responses generated from the machine. 
Drawing intuitions from the Turing test, we can say that a machine-generated response is of higher quality if it is able to fooling an evaluator into believing that it is generated from a human, or in other words, 
a machine-generated response 
 is indistinguishable from a human generated response. 

Everything sounds good so far, but another issue emerges: who is the evaluator? One direct choice is to ask humans (e.g., Turkers) to do the evaluations, but that might be too costly, time-consuming, and  hard to scale up. Another option is, how about training a machine to be an evaluator, where we train a machine-learning classifier to distinguish between machine-generated responses and human-generated responses. We call such a strategy {\bf adversarial evaluation}. Such a strategy also relates to the idea of adversarial training in image generation [4]. 

Adversarial evaluation involves both training
and testing. At training time, the evaluator is
trained to label dialogues as machine-generated
(negative) or human-generated (positive). 
{\bf The evaluator is thus a binary classifier}.
At test
time, the trained evaluator is evaluated on a heldout
dataset. If the human-generated dialogues and
machine-generated ones are indistinguishable, the
model will achieve 50 percent accuracy at test
time.


We define {\bf Adversarial Success} (AdverSuc for
short) to be the fraction of instances in which a
model is capable of fooling the evaluator. AdverSuc
is the difference between 1 and the accuracy
achieved by the evaluator. Higher values of AdverSuc
for a dialogue generation model are better. 
\paragraph{ what you need to do}:
You are given three files, $data/adver.train$, $data/adver.dev$ and $data/adver.test$, which respectively correspond to the training, development and test file. 
Each line in these files take the following forms:
\begin{center}
label $|$ source $|$ target
\end{center}
where label denotes whether the target is generated by a human or a machine (1 indicating being generated by a human and -1 by a machine). 

You can either use the code provided in $q3.py$, or rewrite your own code to train your evaluator. If you plan to use $q3.py$, you need to implement a feature extractor in function $feature\_extractor$. 
Your extracted features need to be outputted to the file $feature\_train.txt$, $feature\_dev.txt$ and $feature\_test.txt$. Each line outputted to  $feature\_train.txt$ should be of the following form:
\begin{center}
label feature1:value feature2:value feature3:value ...
\end{center}
For example, 
\begin{center}
~1 1:3 ~5:1.5 ~7:3.13~ \\
-1 1:2 ~4:1.1 ~8:5~
\end{center}
this means you have two examples, the first instance is a human-generated response (since label is 1). The first instance consists of three features, with index 1, 5, 7, the values of which 
are respectively 3, 1.5, 3.13. Similarly, the second instance is a machine generated response, and consists of three features with index 1, 4, 8 with value of 2, 1.1, 5.
{\bf You can extract whichever feature you want, for example, unigrams within responses}. 
Within each line, the indexes of features should be in ascending order. 
Therefore,
you will encounter an error if your features are as follows: 
$~1~~5:3~~1:1.5$ since your feature indexes are not in ascending order. 
{\bf Feature index starts from 1, not 0}.
File $data/feature$\_$example.txt$ provides an example of what your feature file should be like. 

When you are done with  feature extracting, you can train your classifier using the svm-light package. 
Navigate to the directory svm$\_$light and run the following script:
\begin{center}
cd svm$\_$light \\
./svm$\_$learn ../data/adver.train model \\
\end{center} 
After you are done with training, test your model on your development set and test set:
\begin{center}
./svm$\_$classify ../data/adver.dev model output
\end{center} 
report Adversarial Success on both dev and test sets, which is the difference between 1 and the accuracy you get from svm. 

You are also encouraged to design your own models, using whichever architecture you like (e.g., neural net models), in which case you don't need to use the svm package. 
If that is the case, please wrap up your code for submission. 

Write a few sentences about the potential drawbacks of {\it adversarial evaluation}. 


~~\\~~\\
\noindent{\Large{\bf References}}\\
\noindent [1] Papineni et al., BLEU: a method for automatic evaluation of machine translation. {\it ACL} 2002. \\
\noindent [2] Pennington et al., Pennington, Jeffrey and Socher, Richard and Manning, Christopher D. {\it EMNLP}, 2014. \\
\noindent [3] Sutskever et al., Sequence to Sequence Learning
with Neural Networks. {\it NIPS}, 2014. \\ 
\noindent [4] Goodfellow et al., Generative adversarial nets. {\it NIPS}, 2014. 
\end{document} 



